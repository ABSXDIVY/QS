import requests
import json
import pandas as pd
import time
import re
from sqlalchemy import create_engine, text, exc
from datetime import datetime, date
from urllib.parse import urlencode

# æ•°æ®åº“é…ç½®
DB_CONFIG = {
    'host': 'localhost',
    'user': 'root',
    'password': 'xjtuse',
    'database': 'qs',
    'port': 3306,
    'charset': 'utf8mb4',
    'collation': 'utf8mb4_unicode_ci'
}

def get_database_engine():
    """åˆ›å»ºæ•°æ®åº“è¿æ¥"""
    try:
        engine = create_engine(
            f"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:"
            f"{DB_CONFIG['port']}/{DB_CONFIG['database']}?charset={DB_CONFIG['charset']}"
        )
        with engine.connect():
            print("æ•°æ®åº“è¿æ¥æˆåŠŸ")
        return engine
    except exc.SQLAlchemyError as e:
        print(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
        return None

def create_table_if_not_exists(engine):
    """åˆ›å»ºæ•°æ®è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
    create_table_sql = """
    CREATE TABLE IF NOT EXISTS university_rank (
        id INT AUTO_INCREMENT PRIMARY KEY,
        university_name VARCHAR(255) NOT NULL,
        `rank` INT NOT NULL,
        overall_score DECIMAL(10, 2),
        location VARCHAR(255),
        country VARCHAR(255),
        city VARCHAR(255),
        region VARCHAR(255),
        logo_url TEXT,
        path VARCHAR(500),
        crawl_date DATE NOT NULL,
        create_time DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
        INDEX idx_crawl_date (crawl_date),
        INDEX idx_university (university_name),
        INDEX idx_rank (`rank`),
        INDEX idx_country (country)
    ) ENGINE = InnoDB DEFAULT CHARSET = :charset COLLATE = :collation
    """
    
    try:
        with engine.connect() as conn:
            conn.execute(text(create_table_sql), {
                "charset": DB_CONFIG['charset'],
                "collation": DB_CONFIG['collation']
            })
            conn.commit()
            print("æ•°æ®è¡¨æ£€æŸ¥/åˆ›å»ºå®Œæˆ")
    except exc.SQLAlchemyError as e:
        print(f"è¡¨æ“ä½œå¤±è´¥: {str(e)}")
        raise

def get_session():
    """åˆ›å»ºrequestsä¼šè¯ï¼Œè®¾ç½®è¯·æ±‚å¤´"""
    session = requests.Session()
    
    # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Referer': 'https://www.topuniversities.com/world-university-rankings/2025',
        'Sec-Fetch-Dest': 'empty',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Site': 'same-origin',
        'Cache-Control': 'no-cache',
        'Pragma': 'no-cache'
    }
    
    session.headers.update(headers)
    return session

def extract_data_from_api(session, items_per_page=30):
    """ä»APIæ¥å£æå–æ•°æ®"""
    universities = []
    
    try:
        # QSæ’åAPIæ¥å£
        api_url = "https://www.topuniversities.com/rankings/endpoint"
        
        # è¯·æ±‚å‚æ•°
        params = {
            'nid': '3990755',
            'page': 0,
            'items_per_page': items_per_page,
            'tab': 'indicators',
            'region': '',
            'countries': '',
            'cities': '',
            'search': '',
            'star': '',
            'sort_by': '',
            'order_by': '',
            'program_type': '',
            'scholarship': '',
            'fee': '',
            'english_score': '',
            'academic_score': '',
            'mix_student': '',
            'loggedincache': '6905039-1754356589358'
        }
        
        print(f"æ­£åœ¨è¯·æ±‚API: {api_url}")
        print(f"è¯·æ±‚å‚æ•°: {params}")
        
        response = session.get(api_url, params=params, timeout=30)
        
        if response.status_code == 200:
            try:
                data = response.json()
                print(f"APIå“åº”æˆåŠŸï¼Œæ•°æ®ç±»å‹: {type(data)}")
                print(f"å“åº”é”®: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
                
                # è§£æJSONæ•°æ®
                if 'score_nodes' in data:
                    universities_data = data['score_nodes']
                    print(f"æ‰¾åˆ° {len(universities_data)} ä¸ªå¤§å­¦æ•°æ®")
                    
                    for uni in universities_data:
                        try:
                            # æå–åŸºæœ¬ä¿¡æ¯
                            university_info = {
                                'rank': int(uni.get('rank', 0)),
                                'overall_score': float(uni.get('overall_score', 0)) if uni.get('overall_score') else None,
                                'university_name': uni.get('title', ''),
                                'location': f"{uni.get('city', '')}, {uni.get('country', '')}".strip(', '),
                                'country': uni.get('country', ''),
                                'city': uni.get('city', ''),
                                'region': uni.get('region', ''),
                                'logo_url': uni.get('logo', ''),
                                'path': uni.get('path', '')
                            }
                            
                            universities.append(university_info)
                            
                        except Exception as e:
                            print(f"è§£æå•æ¡æ•°æ®å¤±è´¥: {e}")
                            continue
                    
                    print(f"ä»APIæˆåŠŸæå– {len(universities)} æ¡æ•°æ®")
                    
                else:
                    print("æœªæ‰¾åˆ° 'score_nodes' é”®")
                    print(f"å¯ç”¨çš„é”®: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
                    return pd.DataFrame()
                
            except json.JSONDecodeError as e:
                print(f"JSONè§£æå¤±è´¥: {e}")
                print(f"å“åº”å†…å®¹: {response.text[:500]}...")
                return pd.DataFrame()
                
        else:
            print(f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            print(f"å“åº”å†…å®¹: {response.text[:200]}...")
            
    except requests.exceptions.RequestException as e:
        print(f"è¯·æ±‚å¼‚å¸¸: {e}")
    except Exception as e:
        print(f"æ•°æ®æå–å¤±è´¥: {e}")
    
    return pd.DataFrame(universities)

def extract_data_from_js_url(session, js_url):
    """ä»JSåŠ¨æ€åŠ è½½çš„URLæå–æ•°æ®"""
    universities = []
    
    try:
        print(f"æ­£åœ¨è¯·æ±‚JSæ•°æ®: {js_url}")
        response = session.get(js_url, timeout=30)
        
        if response.status_code == 200:
            # å°è¯•è§£æJSON
            try:
                data = response.json()
                print(f"JSæ•°æ®è§£ææˆåŠŸï¼Œæ•°æ®ç±»å‹: {type(data)}")
                
                # æ ¹æ®å®é™…æ•°æ®ç»“æ„è§£æ
                if isinstance(data, dict) and 'score_nodes' in data:
                    universities_data = data['score_nodes']
                    print(f"æ‰¾åˆ° {len(universities_data)} ä¸ªå¤§å­¦æ•°æ®")
                    
                    for uni in universities_data:
                        try:
                            # æå–åŸºæœ¬ä¿¡æ¯
                            university_info = {
                                'rank': int(uni.get('rank', 0)),
                                'overall_score': float(uni.get('overall_score', 0)) if uni.get('overall_score') else None,
                                'university_name': uni.get('title', ''),
                                'location': f"{uni.get('city', '')}, {uni.get('country', '')}".strip(', '),
                                'country': uni.get('country', ''),
                                'city': uni.get('city', ''),
                                'region': uni.get('region', ''),
                                'logo_url': uni.get('logo', ''),
                                'path': uni.get('path', '')
                            }
                            
                            universities.append(university_info)
                            
                        except Exception as e:
                            print(f"è§£æå•æ¡æ•°æ®å¤±è´¥: {e}")
                            continue
                    
                    print(f"ä»JS URLæˆåŠŸæå– {len(universities)} æ¡æ•°æ®")
                    
                else:
                    print("æ•°æ®ç»“æ„ä¸ç¬¦åˆé¢„æœŸ")
                    print(f"å¯ç”¨çš„é”®: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
                    return pd.DataFrame()
                
            except json.JSONDecodeError as e:
                print(f"JSONè§£æå¤±è´¥: {e}")
                print(f"å“åº”å†…å®¹: {response.text[:200]}...")
                return pd.DataFrame()
                
        else:
            print(f"JSè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            print(f"å“åº”å†…å®¹: {response.text[:200]}...")
            
    except requests.exceptions.RequestException as e:
        print(f"è¯·æ±‚å¼‚å¸¸: {e}")
    except Exception as e:
        print(f"æ•°æ®æå–å¤±è´¥: {e}")
    
    return pd.DataFrame(universities)

def save_to_database(df, engine, crawl_date):
    """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“"""
    if df.empty or engine is None:
        print("æ— æ•°æ®å¯ä¿å­˜")
        return False
    
    df['crawl_date'] = crawl_date
    
    try:
        with engine.connect() as conn:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¯¥æ—¥æœŸçš„æ•°æ®
            result = conn.execute(
                text("SELECT COUNT(*) FROM university_rank WHERE crawl_date = :date"),
                {"date": crawl_date}
            )
            if result.scalar() > 0:
                print(f"è¯¥æ—¥æœŸ({crawl_date})å·²å­˜åœ¨æ•°æ®")
                return False
            
            # ç›´æ¥æ’å…¥æ•°æ®
            df.to_sql('university_rank', conn, if_exists='append', index=False)
            conn.commit()
            print(f"æˆåŠŸä¿å­˜ {len(df)} æ¡æ•°æ®åˆ°æ•°æ®åº“")
            return True
    except exc.SQLAlchemyError as e:
        print(f"æ•°æ®åº“ä¿å­˜å¤±è´¥: {str(e)}")
        return False

def scrape_qs_rankings_requests(js_url=None, items_per_page=30):
    """ä½¿ç”¨requestsçˆ¬å–QSå¤§å­¦æ’åæ•°æ®"""
    # åˆå§‹åŒ–æ•°æ®åº“
    engine = get_database_engine()
    if not engine:
        return
    
    create_table_if_not_exists(engine)
    
    # è®¾ç½®çˆ¬å–æ—¥æœŸ
    crawl_date = date.today().replace(day=1)
    print(f"çˆ¬å–æ—¥æœŸ: {crawl_date}")
    
    # åˆ›å»ºä¼šè¯
    session = get_session()
    
    try:
        df = pd.DataFrame()
        
        if js_url:
            # ä½¿ç”¨æä¾›çš„JS URL
            print("ä½¿ç”¨æä¾›çš„JS URLæå–æ•°æ®...")
            df = extract_data_from_js_url(session, js_url)
        else:
            # å°è¯•APIæ¥å£
            print("å°è¯•APIæ¥å£æå–æ•°æ®...")
            df = extract_data_from_api(session, items_per_page)
        
        if not df.empty:
            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
            print("\nğŸ“Š æ•°æ®é¢„è§ˆ:")
            print(df.head())
            
            # ä¿å­˜åˆ°Excel
            excel_filename = f'QSå¤§å­¦æ’å_requests_{crawl_date}.xlsx'
            df.to_excel(excel_filename, index=False)
            print(f"æ•°æ®å·²ä¿å­˜åˆ° {excel_filename}")
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            save_to_database(df, engine, crawl_date)
        else:
            print("æœªæå–åˆ°ä»»ä½•æ•°æ®")
            
    except Exception as e:
        print(f"çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
    finally:
        session.close()

if __name__ == "__main__":
    # å¦‚æœæ‚¨æœ‰JS URLï¼Œè¯·åœ¨è¿™é‡Œæä¾›
    js_url = "https://www.topuniversities.com/rankings/endpoint?nid=3990755&page=0&items_per_page=30&tab=indicators&region=&countries=&cities=&search=&star=&sort_by=&order_by=&program_type=&scholarship=&fee=&english_score=&academic_score=&mix_student=&loggedincache=6905039-1754356589358"  # ä¾‹å¦‚: "https://api.example.com/rankings"
    
    scrape_qs_rankings_requests(js_url) 